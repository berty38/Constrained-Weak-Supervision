{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from json import JSONDecoder\n",
    "from functools import partial\n",
    "import json\n",
    "from pprint import pprint\n",
    "# from bs4 import BeautifulSoup\n",
    "# from nltk.tokenize import WordPunctTokenizer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "# import mxnet as mx\n",
    "# from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CROWD SOURCING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../datasets/wordsim/'\n",
    "name = datapath.split('/')[-2]\n",
    "df = pd.read_csv(datapath+'original.tsv', sep='\\t')\n",
    "instances = np.unique(df['orig_id'])\n",
    "crowd_labels = []\n",
    "true_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instid in instances:\n",
    "    inst_df = df.loc[df['orig_id'] == instid]\n",
    "    worker_labels = inst_df['response'].values\n",
    "    worker_labels = worker_labels/10 if name=='wordsim' else worker_labels\n",
    "    crowd_labels.append(worker_labels)\n",
    "    true_labels.append(inst_df['gold'].values[0])\n",
    "\n",
    "crowd_labels = np.round(crowd_labels)\n",
    "true_labels = np.asarray(true_labels)\n",
    "if name=='wordsim':\n",
    "    true_labels[true_labels < 2.5] = 0\n",
    "    true_labels[true_labels > 2.5] = 1\n",
    "assert true_labels.size == crowd_labels.shape[0]\n",
    "\n",
    "# save the crowd & true labels\n",
    "np.save(datapath+'crowd_labels.npy', crowd_labels)\n",
    "np.save(datapath+'true_labels.npy', true_labels)\n",
    "\n",
    "df = pd.read_csv(datapath+'features.txt', sep=\"\\t\", quoting=3, header=None)\n",
    "data_features = []\n",
    "for instid in instances:\n",
    "    inst_df = df.loc[df[0] == instid]\n",
    "    data_features.append(inst_df[1].values[0].split(' '))\n",
    "data_features = np.asarray(data_features).astype(float)\n",
    "np.save(datapath+'data_features.npy', data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worker\n",
       "120    187\n",
       "121    880\n",
       "122     38\n",
       "123      5\n",
       "124    371\n",
       "      ... \n",
       "463      9\n",
       "464     84\n",
       "465      4\n",
       "466     32\n",
       "467      7\n",
       "Name: instance, Length: 348, dtype: int64"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['worker'])['instance'].count()[120:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLL DATA & PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "def decodeHTMLencoding(tweets):\n",
    "    decoded_tweets = tweets.applymap(lambda tweet: BeautifulSoup(tweet, 'lxml').get_text())\n",
    "    return decoded_tweets\n",
    "\n",
    "def removeStopWords(text):\n",
    "    stopw = stopwords.words('english')\n",
    "    words = [word for word in text.split() if len(word) > 3 and not word in stopw]\n",
    "    # get stems from words\n",
    "    for i in range(len(words)):\n",
    "        words[i] = stemmer.stem(words[i])\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "def cleanTweets(tweets):\n",
    "    # decode tweets from html tags\n",
    "    cleaned_tweets = decodeHTMLencoding(tweets)\n",
    "    # remove URLs that starts with http\n",
    "    cleaned_tweets = cleaned_tweets.applymap(lambda tweet: re.sub(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', tweet, flags=re.MULTILINE) )\n",
    "    # remove URLs that does not start with http\n",
    "    cleaned_tweets = cleaned_tweets.applymap(lambda tweet: re.sub(\n",
    "    r'[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', tweet, flags=re.MULTILINE))\n",
    "    # remove @\n",
    "    cleaned_tweets = cleaned_tweets.applymap( lambda tweet: re.sub(r'@[A-Za-z0-9_]+', '', tweet, flags=re.MULTILINE) )\n",
    "    # remove #\n",
    "    cleaned_tweets = cleaned_tweets.applymap( lambda tweet: re.sub(r'#[A-Za-z0-9_]+', '', tweet, flags=re.MULTILINE) )\n",
    "    # remove RT\n",
    "    cleaned_tweets = cleaned_tweets.applymap( lambda tweet: re.sub('RT ', '', tweet, flags=re.MULTILINE) )\n",
    "    # remove symbols and numbers (i.e keep letters only)\n",
    "    cleaned_tweets = cleaned_tweets.applymap( lambda tweet: re.sub(\"[^a-zA-Z]\",\" \",tweet, flags=re.MULTILINE) )\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    cleaned_tweets = cleaned_tweets.applymap( lambda tweet: re.sub(r'[^\\x00-\\x7F]+',\" \",tweet.lower(), flags=re.MULTILINE) )\n",
    "    \n",
    "    cleaned_tweets.drop_duplicates(inplace=True)\n",
    "    cleaned_tweets.replace('', np.nan, inplace=True)\n",
    "    cleaned_tweets.dropna(inplace=True)\n",
    "    \n",
    "    return cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(tweets, model):\n",
    "    # dataset should be a pandas dataframe\n",
    "    dimension = 300\n",
    "    data_array = np.empty(shape=[0, dimension])\n",
    "    indexes = []\n",
    "    \n",
    "    for i, tweet in enumerate(tweets):\n",
    "        words = tweet.split()\n",
    "        if len(words) !=0:\n",
    "            feature = 0\n",
    "            for word in words:\n",
    "                try:\n",
    "                    feature += model[word]\n",
    "                except:\n",
    "                    pass\n",
    "            feature /= len(words)\n",
    "            try:\n",
    "                if feature.size == dimension:  \n",
    "                    data_array = np.append(data_array, [feature], axis=0)\n",
    "                    indexes.append(i)\n",
    "            except:\n",
    "                continue\n",
    "    indexes = np.asarray(indexes)\n",
    "    assert indexes.size == data_array.shape[0]\n",
    "    return data_array, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_indices(weak_signals):\n",
    "    # remove indexes of tweets that do not have coverage\n",
    "    indices = np.where(np.sum(weak_signals, axis=1) == -1*weak_signals.shape[1])[0]\n",
    "    weak_signals = np.delete(weak_signals, indices, axis=0)\n",
    "    \n",
    "    return weak_signals, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/glove.42B.300d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove_model = {key: val.values for key, val in df.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9358371614102348"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test word vectors\n",
    "from scipy import spatial\n",
    "result = 1 - spatial.distance.cosine(glove_model['horrible'], glove_model['terrible'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_labeling(data, keywords, sentiment='pos'):\n",
    "    mask = 1 if sentiment == 'pos' else 0\n",
    "    weak_signals = []\n",
    "    for terms in keywords:\n",
    "        weak_signal = []\n",
    "        for text in data:\n",
    "            label=-1\n",
    "            for word in terms:\n",
    "                if word in text.lower():\n",
    "                    label = mask\n",
    "            weak_signal.append(label)\n",
    "        weak_signals.append(weak_signal)\n",
    "    return np.asarray(weak_signals).T\n",
    "\n",
    "POSITIVE_LABELS =  [['good','great','nice','delight','wonderful'], \n",
    "                    ['love', 'best', 'genuine','well', 'thriller'], \n",
    "                    ['clever','enjoy','fine','deliver','fascinating'], \n",
    "                    ['super','excellent','charming','pleasure','strong'], \n",
    "                    ['fresh','comedy', 'interesting','fun','entertain', 'charm', 'clever'], \n",
    "                    ['amazing','romantic','intelligent','classic','stunning'],\n",
    "                    ['rich','compelling','delicious', 'intriguing','smart']]\n",
    "\n",
    "NEGATIVE_LABELS = [['bad','better','leave','never','disaster'], \n",
    "                   ['nothing','action','fail','suck','difficult'], \n",
    "                   ['mess','dull','dumb', 'bland','outrageous'], \n",
    "                   ['slow', 'terrible', 'boring', 'insult','weird','damn'],\n",
    "#                    ['drag','awful','waste', 'flat','worse'],\n",
    "                   ['drag','no','not','awful','waste', 'flat'], \n",
    "                   ['horrible','ridiculous','stupid', 'annoying','painful'], \n",
    "                   ['poor','pathetic','pointless','offensive','silly']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use deep models (1D cov-net or bi-lstm to run experiments)\n",
    "# from keras.preprocessing import sequence\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Bidirectional\n",
    "# from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "# from numpy import array\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.models import Model, Input\n",
    "\n",
    "# # fit a tokenizer\n",
    "# def create_tokenizer(lines):\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts(lines)\n",
    "#     return tokenizer\n",
    " \n",
    "# # calculate the maximum document length\n",
    "# def max_length(lines):\n",
    "#     return max([len(s.split()) for s in lines])\n",
    " \n",
    "# # encode a list of lines\n",
    "# def encode_text(tokenizer, lines, length):\n",
    "#     # integer encode\n",
    "#     encoded = tokenizer.texts_to_sequences(lines)\n",
    "#     # pad encoded sequences\n",
    "#     padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "#     return padded\n",
    "\n",
    "# def text_Conv1D(max_features, maxlen):\n",
    "#     filters = 32\n",
    "#     kernel_size = 8\n",
    "#     hidden_dims = 10\n",
    "    \n",
    "#     model = Sequential()\n",
    "\n",
    "#     # we start off with an efficient embedding layer which maps\n",
    "#     # our vocab indices into embedding_dims (100) dimensions\n",
    "#     model.add(Embedding(max_features,\n",
    "#                         100,\n",
    "#                         input_length=maxlen))\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     # we add a Convolution1D, which will learn filters\n",
    "#     # word group filters of size filter_length:\n",
    "#     model.add(Conv1D(filters=filters,\n",
    "#                      kernel_size=kernel_size,\n",
    "#                      padding='valid',\n",
    "#                      activation='relu',\n",
    "#                      strides=1))\n",
    "#     # we use max pooling:\n",
    "#     model.add(GlobalMaxPooling1D())\n",
    "\n",
    "#     # We add a vanilla hidden layer:\n",
    "#     model.add(Dense(hidden_dims))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Activation('relu'))\n",
    "    \n",
    "# #     # use bi-LSTM instead\n",
    "# #     model.add(Bidirectional(LSTM(64)))\n",
    "# #     model.add(Dropout(0.5))\n",
    "    \n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#     model.compile(loss='binary_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['accuracy'])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../datasets/yelp/'\n",
    "size = 10000\n",
    "review = pd.read_json(datapath+'yelp_review.json', lines=True,\n",
    "                      dtype={'review_id':str,'user_id':str,\n",
    "                             'business_id':str,'stars':int,\n",
    "                             'date':str,'text':str,'useful':int,\n",
    "                             'funny':int,'cool':int},\n",
    "                      chunksize=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple chunks to be read\n",
    "count=0\n",
    "chunk_list = []\n",
    "for chunk_review in review:\n",
    "    # Drop columns that aren't needed\n",
    "    chunk_review = chunk_review.drop(['review_id','user_id','useful','funny','cool','business_id','date'], axis=1)\n",
    "    # Renaming column name to avoid conflict with business overall star rating\n",
    "    # chunk_review = chunk_review.rename(columns={'stars': 'review_stars'})\n",
    "    # Inner merge with edited business file so only reviews related to the business remain\n",
    "    # chunk_merged = pd.merge(business_RV, chunk_review, on='business_id', how='inner')\n",
    "    # Show feedback on progress\n",
    "    # print(f\"{chunk_merged.shape[0]} out of {size:,} related reviews\")\n",
    "    # chunk_list.append(chunk_merged)\n",
    "    chunk_list.append(chunk_review)\n",
    "    count +=1\n",
    "    if count==6:\n",
    "        break\n",
    "# After trimming down the review file, concatenate all relevant data back to one dataframe\n",
    "df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>As someone who has worked with many museums, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I am actually horrified this place is still in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Oh happy day, finally have a Canes near my cas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text\n",
       "0      2  As someone who has worked with many museums, I...\n",
       "1      1  I am actually horrified this place is still in...\n",
       "2      5  I love Deagan's. I do. I really do. The atmosp...\n",
       "3      1  Dismal, lukewarm, defrosted-tasting \"TexMex\" g...\n",
       "4      4  Oh happy day, finally have a Canes near my cas..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_name = datapath+\"yelp_reviews.csv\"\n",
    "df.to_csv(csv_name, index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9f1346def728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpositive_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeyword_labeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOSITIVE_LABELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnegative_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeyword_labeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNEGATIVE_LABELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mweak_signals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpositive_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweak_signals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweak_signals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweak_signals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "positive_labels = keyword_labeling(df.text.values, POSITIVE_LABELS, sentiment='pos')\n",
    "negative_labels = keyword_labeling(df.text.values, NEGATIVE_LABELS, sentiment='neg')\n",
    "weak_signals = np.hstack([positive_labels, negative_labels])\n",
    "weak_signals, indices = remove_indices(weak_signals)\n",
    "weak_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.index[indices])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "train_data = df.text.values\n",
    "train_labels = np.zeros(df.shape[0])\n",
    "train_labels[df.stars.values >3]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55370, 1), (55370,))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = cleanTweets(df.drop(columns=['stars']))\n",
    "train_labels = train_labels[train_data.index]\n",
    "weak_signals = weak_signals[train_data.index]\n",
    "train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_index = get_text_vectors(train_data.values.ravel(), glove_model)\n",
    "train_features.shape, train_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test data\n",
    "np.random.seed(5000)\n",
    "test_indexes = np.random.choice(train_index.size, 10000, replace=False)\n",
    "test_labels = train_labels[test_indexes]\n",
    "test_data = train_features[test_indexes]\n",
    "\n",
    "train_data = np.delete(train_features, test_indexes, axis=0)\n",
    "weak_signals = np.delete(weak_signals, test_indexes, axis=0)\n",
    "train_labels = np.delete(train_labels, test_indexes)\n",
    "\n",
    "train_data.shape,train_labels.shape,weak_signals.shape,test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the weak_signals signals\n",
    "np.save(datapath+'weak_signals.npy', weak_signals)\n",
    "\n",
    "# save yelp data\n",
    "np.save(datapath+'data_features.npy', train_data)\n",
    "np.save(datapath+'test_features.npy', test_data)\n",
    "\n",
    "# save yelp labels\n",
    "np.save(datapath+'data_labels.npy', train_labels)\n",
    "np.save(datapath+'test_labels.npy', test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45370, 300), (45370,), (45370, 14), (10000,))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape,train_labels.shape,weak_signals.shape,test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A stirring, funny and finally transporting re-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Apparently reassembled from the cutting-room f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They presume their audience won't sit still fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This is a visually stunning rumination on love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Jonathan Parker's Bartleby should have been th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           sentence\n",
       "0      1  A stirring, funny and finally transporting re-...\n",
       "1      0  Apparently reassembled from the cutting-room f...\n",
       "2      0  They presume their audience won't sit still fo...\n",
       "3      1  This is a visually stunning rumination on love...\n",
       "4      1  Jonathan Parker's Bartleby should have been th..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = '../datasets/sst-2/'\n",
    "train_data = pd.read_csv(datapath+'sst2-train.csv')\n",
    "test_data = pd.read_csv(datapath+'sst2-test.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_labels = keyword_labeling(train_data, POSITIVE_LABELS)\n",
    "negative_labels = keyword_labeling(train_data, NEGATIVE_LABELS, sentiment='neg')\n",
    "weak_signals = np.hstack([positive_labels, negative_labels])\n",
    "weak_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3998, 14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_signals = remove_indices(train_data, weak_signals)\n",
    "weak_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  [[0.30916844]\n",
      " [0.29194631]\n",
      " [0.26710098]\n",
      " [0.29081633]\n",
      " [0.36492375]\n",
      " [0.31952663]\n",
      " [0.19417476]\n",
      " [0.34623218]\n",
      " [0.32853026]\n",
      " [0.2513369 ]\n",
      " [0.33333333]\n",
      " [0.44829801]\n",
      " [0.15116279]\n",
      " [0.18348624]]\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_data.label.values\n",
    "test_labels = test_data.label.values\n",
    "\n",
    "n,m = weak_signals.shape\n",
    "weak_signal_probabilities = weak_signals.T.reshape(m,n,1)\n",
    "\n",
    "weak_signals_mask = weak_signal_probabilities >=0\n",
    "\n",
    "from setup_model import get_validation_bounds\n",
    "true_error_rates, true_precisions = get_validation_bounds(train_labels, weak_signal_probabilities, weak_signals_mask)\n",
    "print(\"error: \", np.asarray(true_error_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3998, 1) (3998,)\n",
      "(1821, 1) (1821,)\n"
     ]
    }
   ],
   "source": [
    "# Clean data and reset index\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# apply on train data\n",
    "train_data = cleanTweets(train_data.drop(columns=['label']))\n",
    "train_data = post_process_tweets(train_data)\n",
    "\n",
    "# apply on test data\n",
    "test_data = cleanTweets(test_data.drop(columns=['label']))\n",
    "test_data = post_process_tweets(test_data)\n",
    "\n",
    "print(train_data[0].shape, train_labels.shape)\n",
    "print(test_data[0].shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       a stirring  funny and finally transporting re ...\n",
       "1       they presume their audience won t sit still fo...\n",
       "2       this is a visually stunning rumination on love...\n",
       "3       campanella gets the tone just right    funny i...\n",
       "4       a fan film that for the uninitiated plays bett...\n",
       "                              ...                        \n",
       "3600    painful  horrifying and oppressively tragic  t...\n",
       "3601    take care is nicely performed by a quintet of ...\n",
       "3602    the script covers huge  heavy topics in a blan...\n",
       "3603    a seriously bad film with seriously warped log...\n",
       "3604    a deliciously nonsensical comedy about a city ...\n",
       "Name: sentence, Length: 3605, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_index = get_text_vectors(train_data[0].values.ravel(), glove_model)\n",
    "test_features, test_index = get_text_vectors(test_data[0].values.ravel(), glove_model)\n",
    "\n",
    "# save sst-2 data\n",
    "np.save(datapath+'data_features.npy', train_features)\n",
    "np.save(datapath+'test_features.npy', test_features)\n",
    "\n",
    "indexes = train_data[1]\n",
    "indexes = indexes[train_index]\n",
    "# save sst-2 labels\n",
    "np.save(datapath+'data_labels.npy', train_labels[indexes])\n",
    "np.save(datapath+'test_labels.npy', test_labels[test_data[1]])\n",
    "\n",
    "# save the one-hot signals\n",
    "np.save(datapath+'weak_signals.npy', weak_signals[indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests on the baselines...\n"
     ]
    }
   ],
   "source": [
    "print(\"Running tests on the baselines...\")\n",
    "baseline_weak_labels = weak_signal_probabilities[:m, :, :]\n",
    "baseline_weak_labels = np.rint(baseline_weak_labels)\n",
    "mv_weak_labels = np.ones(baseline_weak_labels.shape)\n",
    "mv_weak_labels[baseline_weak_labels==-1] =0\n",
    "mv_weak_labels[baseline_weak_labels==0] =-1\n",
    "mv_weak_labels = np.sign(np.sum(mv_weak_labels, axis=0))\n",
    "mv_weak_labels[mv_weak_labels==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 48\n",
      "Vocabulary size: 9830\n",
      "(3605, 48) (1821, 48)\n",
      "Epoch 1/4\n",
      "3605/3605 [==============================] - 14s 4ms/step - loss: 0.6308 - acc: 0.6169\n",
      "Epoch 2/4\n",
      "3605/3605 [==============================] - 12s 3ms/step - loss: 0.1994 - acc: 0.9268\n",
      "Epoch 3/4\n",
      "3605/3605 [==============================] - 13s 3ms/step - loss: 0.0376 - acc: 0.9908\n",
      "Epoch 4/4\n",
      "3605/3605 [==============================] - 12s 3ms/step - loss: 0.0124 - acc: 0.9961\n",
      "Train Accuracy: 69.403606\n",
      "Test Accuracy: 62.108731\n"
     ]
    }
   ],
   "source": [
    "trainLines = train_data[0].sentence\n",
    "testLines = test_data[0].sentence\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "testX = encode_text(tokenizer, testLines, length)\n",
    "print(trainX.shape, testX.shape)\n",
    "\n",
    "model = text_Conv1D(vocab_size, length)\n",
    "\n",
    "model.fit(trainX, mv_weak_labels, batch_size=32, epochs=4)\n",
    " \n",
    "# evaluate model on training dataset\n",
    "loss, acc = model.evaluate(trainX, train_labels, verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    " \n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate(testX,test_labels, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREC-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5452, 2)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_trecdata(filename):    \n",
    "    filepath = '../datasets/trec-6/'\n",
    "    labels = []\n",
    "    data = []\n",
    "    for line in open(filepath+filename, encoding = \"ISO-8859-1\"):\n",
    "        temp = line.rstrip().split(' ')\n",
    "        labels.append(temp[0])\n",
    "        data.append(' '.join(temp[1:]))\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# read in train data\n",
    "train_data = read_trecdata('trec_traindata.txt')\n",
    "train_data = np.asarray([train_data[0], train_data[1]]).T\n",
    "train_data = pd.DataFrame(train_data, columns=['Text', 'Label'])\n",
    "\n",
    "# read in test data\n",
    "test_data = read_trecdata('trec_testdata.txt')\n",
    "test_data = np.asarray([test_data[0], test_data[1]]).T\n",
    "test_data = pd.DataFrame(test_data, columns=['Text', 'Label'])\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_labels(dataframe, keywords):\n",
    "    questions = dataframe.Text.values\n",
    "    weak_signals = []\n",
    "    for term in keywords:\n",
    "        weak_signal = []\n",
    "        if len(np.asarray(term).shape) ==1:\n",
    "            word, mask = term\n",
    "            for question in questions:\n",
    "                label = mask if word in question.lower() else -1\n",
    "                weak_signal.append(label)\n",
    "        else:\n",
    "            for question in questions:\n",
    "                label=-1\n",
    "                for tag in term:\n",
    "                    word, mask = tag\n",
    "                    if word in question.lower():\n",
    "                        label = mask\n",
    "                weak_signal.append(label)\n",
    "        weak_signals.append(weak_signal)\n",
    "\n",
    "    return weak_signals\n",
    "    \n",
    "def uppercase_signals(dataframe):\n",
    "    questions = dataframe.Text.values\n",
    "    weak_signal = []\n",
    "    for question in questions:\n",
    "        words = question.split(' ')\n",
    "        label= -1\n",
    "        if words[2].isupper() and len(words[2])>1:\n",
    "            label = 2 \n",
    "        weak_signal.append(label)\n",
    "    return weak_signal\n",
    "    \n",
    "def what_signals(dataframe):\n",
    "    questions = dataframe.Text.values\n",
    "    weak_signal = []\n",
    "    for question in questions:\n",
    "        label= -1\n",
    "        if 'what' in question.lower():\n",
    "            label = 1\n",
    "        weak_signal.append(label)\n",
    "    return weak_signal          \n",
    "\n",
    "weak_signal_1 = [[['why',0],['come from',0], ['what is `',0]], \n",
    "                 [['was the name',1],['film',1], ['do you say',1]], \n",
    "                 ['abbreviat',2], ['who',3], [['city',4],['mountain',4]], [['when was',5],['date',5]]]\n",
    "\n",
    "weak_signal_2 = [[['how does',0], ['origin of',0], ['do to',0]],\n",
    "                 [['color of',1], ['sport',1], ['fear of',1],['favorite',1]],\n",
    "                 ['stand for',2],[['writer',3],['wrote',3]], ['where',4],['year',5]]\n",
    "\n",
    "trec_terms_0 = [[['the word',0],['explain',0], ['how can',0], ['difference',0]],\n",
    "                [['name a',1], ['kind of',1], ['term for',1]]]\n",
    "trec_terms_1 = [[['invent',3],['president',3]], [['state',4],['originate',4], ['location',4]], ['how many',5]]\n",
    "\n",
    "weak_signal_4 = [[['mean',0],['known for ?',0],['meaning',0]], [['made of',1],['common',1]],\n",
    "                 ['full form',2],[['name ?',3],['occupation',3]], [['nationality',4],\n",
    "                 ['planet',4]], [['are there ?',5],['have ?',5],['when did',5]]]\n",
    "\n",
    "weak_signal_5 = [[['what causes',0], ['what effect',0], ['definition',0], ['how did',0]],\n",
    "                 [['favorite',1],['test',1], ['animal',1]], ['acronym',2], ['company',3],\n",
    "                [['capital',4],['country',4]],['how much',5]]\n",
    "#['what is a']\n",
    "# get keyword labelings\n",
    "weak_signal_1 = np.asarray(keyword_labels(train_data, weak_signal_1)).T\n",
    "weak_signal_2 = np.asarray(keyword_labels(train_data, weak_signal_2)).T\n",
    "weak_signal_3 = keyword_labels(train_data, trec_terms_0)\n",
    "weak_signal_3.append(uppercase_signals(train_data))\n",
    "weak_signal_4 = np.asarray(keyword_labels(train_data, weak_signal_4)).T\n",
    "weak_signal_5 = np.asarray(keyword_labels(train_data, weak_signal_5)).T\n",
    "# print(np.asarray(uppercase_signals(train_data)).shape)\n",
    "# print(np.asarray(trec_terms_2[0]).shape)\n",
    "\n",
    "weak_signal_3 = np.vstack([weak_signal_3, keyword_labels(train_data, trec_terms_1)]).T\n",
    "weak_signals = np.hstack([weak_signal_1, weak_signal_2, weak_signal_3, weak_signal_4, weak_signal_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2969, 30)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = '../datasets/trec-6/'\n",
    "weak_signals, indices = remove_indices(weak_signals)\n",
    "weak_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the array into two separate signals\n",
    "weak_signals = np.asarray(np.split(weak_signals, int(weak_signals.shape[1]/6), axis=1))\n",
    "\n",
    "# create_one_hot_signals\n",
    "one_hot_signals = np.zeros(weak_signals.shape)\n",
    "one_hot_signals[weak_signals==-1] = -1\n",
    "one_hot_signals[weak_signals!=-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coverage:  [[146 108  21 632 152  77]\n",
      " [101 116  44  52 286 114]\n",
      " [116  93  75 148 135 323]\n",
      " [109  47   1  48  16  92]\n",
      " [ 60  87   4  54 158  61]]\n",
      "error:  [[0.10273973 0.37037037 0.04761905 0.03481013 0.18421053 0.07792208]\n",
      " [0.02970297 0.14655172 0.06818182 0.07692308 0.0979021  0.24561404]\n",
      " [0.04310345 0.22580645 0.45333333 0.24324324 0.33333333 0.        ]\n",
      " [0.20183486 0.34042553 0.         0.29166667 0.0625     0.04347826]\n",
      " [0.01666667 0.28735632 0.         0.18518519 0.08860759 0.06557377]]\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.drop(train_data.index[indices])\n",
    "print(\"coverage: \", np.sum(one_hot_signals != -1, axis=1))\n",
    "\n",
    "train_labels = train_data.Label.values\n",
    "test_labels = test_data.Label.values\n",
    "\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "num_classes = weak_signals.shape[2]\n",
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes)\n",
    "\n",
    "weak_signals_mask = one_hot_signals >=0\n",
    "\n",
    "from setup_model import get_validation_bounds\n",
    "true_error_rates, true_precisions = get_validation_bounds(train_labels, one_hot_signals, weak_signals_mask)\n",
    "print(\"error: \", np.asarray(true_error_rates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2946, 1) (2946, 6)\n",
      "(500, 1) (500, 6)\n"
     ]
    }
   ],
   "source": [
    "# Clean data and reset index\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# apply on train data\n",
    "train_data = cleanTweets(train_data.drop(columns=['Label']))\n",
    "train_labels = train_labels[train_data.index]\n",
    "one_hot_signals = one_hot_signals[:,train_data.index,:]\n",
    "assert train_data.shape[0] == train_labels.shape[0]\n",
    "\n",
    "# apply on test data\n",
    "test_data = cleanTweets(test_data.drop(columns=['Label']))\n",
    "\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2946, 300) (2946,)\n",
      "(500, 300) (500,)\n"
     ]
    }
   ],
   "source": [
    "train_features, train_index = get_text_vectors(train_data.values.ravel(), glove_model)\n",
    "test_features, test_index = get_text_vectors(test_data.values.ravel(), glove_model)\n",
    "\n",
    "print(train_features.shape, train_index.shape)\n",
    "print(test_features.shape, test_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the one-hot signals\n",
    "np.save(datapath+'weak_signals.npy', one_hot_signals)\n",
    "\n",
    "# save trec-6 data\n",
    "np.save(datapath+'data_features.npy', train_features)\n",
    "np.save(datapath+'test_features.npy', test_features)\n",
    "\n",
    "# save trec-6 labels\n",
    "np.save(datapath+'data_labels.npy', train_labels)\n",
    "np.save(datapath+'test_labels.npy', test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 2), 49580)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = '../datasets/imbd/'\n",
    "df = pd.read_csv(datapath+'IMDB Dataset.csv')\n",
    "\n",
    "# apply on train data\n",
    "cleaned_data = cleanTweets(df.drop(columns=['sentiment']))\n",
    "indexes = cleaned_data.index.values\n",
    "df.shape, indexes.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39664,) (39664,)\n",
      "(9916,) (9916,)\n"
     ]
    }
   ],
   "source": [
    "n = indexes.size\n",
    "# get test data\n",
    "np.random.seed(50)\n",
    "test_indexes = np.random.choice(indexes, int(n*0.2), replace=False)\n",
    "test_labels = np.zeros(test_indexes.size)\n",
    "test_labels[df.sentiment.values[test_indexes]=='positive'] = 1\n",
    "test_data = df.review.values[test_indexes]\n",
    "\n",
    "train_indexes = np.delete(indexes, [np.where(indexes == i)[0][0] for i in test_indexes])\n",
    "train_labels = np.zeros(train_indexes.size)\n",
    "train_labels[df.sentiment.values[train_indexes]=='positive'] = 1\n",
    "train_data = df.review.values[train_indexes]\n",
    "\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29187, 10)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_labels = keyword_labeling(train_data, [['good'],['wonderful'],['great'],['amazing'],['excellent']], sentiment='pos')\n",
    "negative_labels = keyword_labeling(train_data, [['bad'],['horrible'],['sucks'],['awful'],['terrible']], sentiment='neg')\n",
    "# signals = np.add(positive_labels, negative_labels)\n",
    "# weak_signals = np.ones(signals.shape)\n",
    "# weak_signals[signals==-1] =0 \n",
    "# weak_signals[signals==0] =-1\n",
    "weak_signals = np.hstack([positive_labels, negative_labels])\n",
    "weak_signals, indices = remove_indices(weak_signals)\n",
    "weak_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with bert embeddings\n",
    "bert_abstract = \"\"\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.\n",
    " Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.\n",
    " As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \n",
    "BERT is conceptually simple and empirically powerful. \n",
    "It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%.\"\"\"\n",
    "# sentences = bert_abstract.split('\\n')\n",
    "# bert_embedding = BertEmbedding()\n",
    "# result = bert_embedding(sentences)\n",
    "\n",
    "# sentences = df.review.values\n",
    "# result = bert_embedding(sentences)\n",
    "\n",
    "# data_features = []\n",
    "# for embedding in result:\n",
    "#     data_features.append(np.mean(embedding[1],axis=0))\n",
    "# # save imbd data\n",
    "# np.save(datapath+'data_features.npy', np.asarray(data_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29182 29187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20392, 20393)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add signals not covered to test data\n",
    "test_data = np.append(test_data, train_data[indices])\n",
    "test_labels = np.append(test_labels, train_labels[indices])\n",
    "\n",
    "# delete train data not covered by weak signals\n",
    "train_data = np.delete(train_data, indices, axis=0)\n",
    "train_labels = np.delete(train_labels, indices)\n",
    "\n",
    "# get data features\n",
    "train_features, train_index = get_text_vectors(train_data, glove_model)\n",
    "test_features, test_index = get_text_vectors(test_data, glove_model)\n",
    "\n",
    "print(train_index.size, train_data.shape[0])\n",
    "test_index.size, test_labels.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests on the baselines...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7361387156466315"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Running tests on the baselines...\")\n",
    "baseline_weak_labels = weak_signals.copy()\n",
    "mv_weak_labels = np.ones(baseline_weak_labels.shape)\n",
    "mv_weak_labels[baseline_weak_labels==-1] =0\n",
    "mv_weak_labels[baseline_weak_labels==0] =-1\n",
    "mv_weak_labels = np.sign(np.sum(mv_weak_labels, axis=1))\n",
    "mv_weak_labels[mv_weak_labels==-1] = 0\n",
    "\n",
    "np.mean(mv_weak_labels[train_index]==train_labels[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save imbd data\n",
    "np.save(datapath+'data_features.npy', train_features)\n",
    "np.save(datapath+'test_features.npy', test_features)\n",
    "\n",
    "# save imbd labels\n",
    "np.save(datapath+'data_labels.npy', train_labels[train_index])\n",
    "np.save(datapath+'test_labels.npy', test_labels[test_index])\n",
    "\n",
    "# save the weak_signals\n",
    "np.save(datapath+'weak_signals.npy', weak_signals[train_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
